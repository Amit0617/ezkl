\documentclass[12pt]{amsart}  
%\documentclass[12pt]{article}  

\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{alltt}
\usepackage{mathrsfs} 
\usepackage{amsfonts}
\usepackage{amssymb}%
\usepackage{setspace}
%\usepackage{mathtools}%for \coloneqq
\setcounter{MaxMatrixCols}{30}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{url}
\usepackage[all,knot,poly]{xypic} 
\usepackage{endnotes}
\usepackage[margin=1.5in]{geometry} 
%\usepackage{pstricks}
\usepackage{tikz}
\usetikzlibrary{plotmarks}

\usetikzlibrary{decorations.markings}


%\setlength{\parskip}{1cm plus4mm minus3mm}
\setlength{\parskip}{2mm plus1mm minus1mm}

\newcommand{\tblue}[1]{\textcolor{blue}{#1}}


%\usetikzlibrary{arrows,backgrounds,positioning,fit } 
\usepackage{enumerate} %http://www.tex.ac.uk/cgi-bin/texfaq2html?label=enumerate 
%\usepackage{tikzpicture}
%\usepackage[tiling]{pst-fill}
\usepackage{comment} %http://tug.ctan.org/cgi-bin/ctanPackageInformation.py?id=comment
\newcommand{\E}{\mathbb{E}} 
\newcommand{\B}{\mathbb{B}} 
\newcommand{\mcA}{\mathcal{A}}
\newcommand{\mcB}{\mathcal{B}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcD}{\mathcal{D}}
\newcommand{\mcE}{\mathcal{E}}   
\newcommand{\mcF}{\mathcal{F}}
\newcommand{\mcL}{\mathcal{L}} 
\newcommand{\mcM}{\mathcal{M}} 
\newcommand{\mcO}{\mathcal{O}} 
\newcommand{\mcP}{\mathcal{P}}
\newcommand{\mcQ}{\mathcal{Q}}
\newcommand{\mcK}{\mathcal{K}}
\newcommand{\mcS}{\mathcal{S}}
\newcommand{\mcT}{\mathcal{T}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\mcW}{\mathcal{W}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\setA}{\mathtt{A}}
\newcommand{\setB}{\mathtt{B}}
\newcommand{\setC}{\mathtt{C}}
\newcommand{\setD}{\mathtt{D}}
\newcommand{\setX}{\mathtt{X}}
\newcommand{\setY}{\mathtt{Y}}
\newcommand{\setf}{\mathtt{f}}
\newcommand{\frkp}{\mathfrak{p}}
\newcommand{\frkq}{\mathfrak{q}}
\newcommand{\frkr}{\mathfrak{r}}
\newcommand{\scrC}{\mathscr{C}} 
\newcommand{\scrD}{\mathscr{D}}
\newcommand{\scrF}{\mathscr{F}} 
\newcommand{\scrG}{\mathscr{G}} 
\newcommand{\scrR}{\mathscr{R}} 
\newcommand{\scrS}{\mathscr{S}} 
\newcommand{\scrT}{\mathscr{T}} 
\newcommand{\scrJ}{\mathscr{J}}
\newcommand{\scrP}{\mathscr{P}}
\newcommand{\scrQ}{\mathscr{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}} 
\newcommand{\N}{\mathbb{N}} 
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\bfA}{\mathbf{A}} 
\newcommand{\bfB}{\mathbf{B}}  
\newcommand{\bfX}{\mathbf{X}} 
\newcommand{\bfY}{\mathbf{Y}} 
\newcommand{\bfZ}{\mathbf{Z}} 
\newcommand{\bfW}{\mathbf{W}} 
\newcommand{\bfC}{\mathbf{C}} 
\newcommand{\bfD}{\mathbf{D}} 
\newcommand{\bfM}{\mathbf{M}} 
\newcommand{\y}{\mathbf{y}}   
\newcommand{\ulf}{\underline{f}} 
\newcommand{\ulg}{\underline{g}} 
\newcommand{\bff}{\mathbf{f}}
\newcommand{\K}{\mathbf{k}}
\newcommand{\KK}{\mathbf{k}}
\newcommand{\bfg}{\mathbf{g}}
\newcommand{\stardagger}{\dagger\!\!\!{}^*}
\newcommand{\x}{\mathbf{x}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\isom}{\cong}
\newcommand{\size}{\operatorname{size}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\lift}{\operatorname{lift}}
\newcommand{\nf}{\operatorname{nf}}
\newcommand{\isomto}{\stackrel{\sim}{\rightarrow}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ot}{\otimes}
\newcommand{\otc}{\otimes \cdots \otimes}
\newcommand{\ote}{\!\otimes\!}
\newcommand{\botimes}{\bar{\otimes}}
\newcommand{\bcirc}{\bar{\circ}}
\newcommand{\by}{\! \times \!}
\newcommand{\ra}{\rightarrow}
\newcommand{\raf}{\stackrel{f}{\rightarrow}}
\newcommand{\rag}{\stackrel{g}{\rightarrow}}
\newcommand{\rafe}{\!\stackrel{f}{\rightarrow}\!}
\newcommand{\fab}{f\!:\!A\!\rightarrow\!B}
\newcommand{\gbc}{f\!:\!B\!\rightarrow\!C}
\newcommand{\rae}{\!\rightarrow\!}
\newcommand{\Sec}{\operatorname{Sec}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Spin}{\operatorname{Spin}}
\newcommand{\val}{\operatorname{val}}
\newcommand{\nbhd}{\operatorname{nbhd}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\PGL}{\operatorname{PGL}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\eval}{\operatorname{eval}}
\newcommand{\argmax}{\operatorname{argmax}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\Id}{\operatorname{Id}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\ceil}[1]{\ensuremath{\lceil #1 \rceil}}
\newcommand{\floor}[1]{\ensuremath{\lfloor #1 \rfloor}}
\newcommand{\are}[3]{ #1 \! : \! #2 \! \rightarrow \! #3 }
\newcommand{\PS}{\C\{\!\{t\}\!\}}
\newcommand{\Sn}{\mathbb{S}}
\newcommand {\TP} {\mathbb{TP}}
\newcommand{\un}{\underline{n}}
\newcommand{\on}{\bar{n}}
\newcommand{\sPf}{\operatorname{sPf}}
\newcommand{\sPfd}{\operatorname{sPf}^{\vee}}
\newcommand{\Pf}{\operatorname{Pf}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\sspan}{\operatorname{span}}
\newcommand{\sDet}{\operatorname{sDet}}
\newcommand{\MaxsDet}{\operatorname{MaxsDet}}
\newcommand{\union}{\cup}
\newcommand{\Pol}{\operatorname{Pol}}
\newcommand{\Inv}{\operatorname{Inv}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\Aff}{\operatorname{Aff}}
\newcommand{\Lift}{\operatorname{Lift}}
\newcommand{\Supp}{\operatorname{Supp}}
\newcommand{\Conv}{\operatorname{Conv}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Mod}{\operatorname{Mod}}
\newcommand{\ModSet}{\operatorname{ModSet}}
\newcommand{\ModPrime}{\operatorname{ModPrime}}
\newcommand {\unit} {1\!\!1}
\newcommand{\pa}{\operatorname{pa}}



\newcommand{\ssum}{\operatorname{ssum}}

%Categories 
\newcommand{\Ob}{\operatorname{Ob}}
\newcommand{\Mon}{\operatorname{Mon}}
\newcommand{\Mor}{\operatorname{Mor}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\xlang}{\mcT_{X}^{\ot,\circ}}

\newcommand{\PSM}{\operatorname{PSM}}
\newcommand{\ST}{\mathcal{ST}}
\newcommand{\OP}{\mathcal{OP}}

%\newcommand{\Pow}{\operatorname{Pow}}
%\newcommand{\cPow}{\overline{\operatorname{Pow}}}
\newcommand{\Pow}{\mcP}
\newcommand{\cPow}{\bar{\mcP}}
\newcommand{\Ten}{\operatorname{Ten}}
\newcommand{\cod}{\operatorname{cod}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\Interp}{\operatorname{Interp}}
\newcommand{\Vect}{{\rm Vect}}
\newcommand{\Top}{{\rm Top}} 
\newcommand{\Set}{{\rm Set}} 
\newcommand{\Rel}{{\rm Rel}}
\newcommand{\Mat}{\operatorname{Mat}}
\newcommand{\BTRel}{{\rm BTRel}} 
\newcommand{\BoolRel}{{\rm BoolRel}} 
\newcommand{\LTRel}{{\rm LTRel}}
\newcommand{\DAGSimp}{{\rm DAGSimp}}
\newcommand{\SPTN}{{\rm SPTN}}
\newcommand{\StrMonFun}{{\rm StrMonFun}}
\newcommand{\GraphHM}{{\rm GraphHM}}
\newcommand{\FVFV}{{\rm FVFV}}
\newcommand{\FVL}{{\rm FVL}}
\newcommand{\FVS}{{\rm FVS}}
\newcommand{\FVA}{{\rm FVA}}
\newcommand{\FVP}{{\rm FVP}}
\newcommand{\QC}{{\rm QC}}
\newcommand{\UFGMPt}{{\rm UFGM\text{-}Pt}}
\newcommand{\UFGMVar}{{\rm UFGM\text{-}Var}}
\newcommand{\UFGMRing}{{\rm UFGM\text{-}Ring}}
\newcommand{\FinRel}{{\rm FinRel}}
\newcommand{\NFinRel}{\N{\rm FinRel}}

\newcommand{\cpy}{\operatorname{copy}}
\newcommand{\counit}{\operatorname{counit}}
\newcommand{\unt}{\operatorname{unit}}
\newcommand{\coev}{\operatorname{coev}} 
\newcommand{\ev}{\operatorname{ev}}

%
\newcommand{\NA}{\operatorname{\tiny NA}}


%Boolean gates
\newcommand{\NAND}{\operatorname{NAND}}


%bra-ket
\newcommand{\bra}{\langle} 
\newcommand{\ket}{\rangle} 
\newcommand{\la}{\langle} 

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}%[section]
\theoremstyle{definition}
\newtheorem{notation}[theorem]{Notation}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{definition}
\newtheorem{algorithm}{Algorithm}



\specialcomment{issue}
{\begingroup\color{red}\footnotesize}{\endgroup}

\excludecomment{baa}
\excludecomment{issue} 
\excludecomment{comment}

%\includecomment{comment} %uses comment package.  For pedantic details, exercises, working out stuff that will get cut from the journal version.
%\excludecomment{nicetohave}
%\excludecomment{comment}


\tikzstyle{dom}=[rectangle,minimum height=20pt,minimum width=18pt,draw, fill=blue!20]
\tikzstyle{cod}=[minimum height=20pt,minimum width=18pt,draw, fill=red!20]
%\tikzstyle{dombox}=[(\draw [blue, fill=blue] (4,0) -- (5,1) -- (4.75,0.15) -- (4,0);)]

\bibliographystyle{custom}

\title[zkdlh2]{ZKDLH2}

%\author[Jason Morton]{Jason  Morton} %alphabetical 
%\thanks{${}^*$Departments of Mathematics and Statistics, Pennsylvania State University.}


\begin{document}
\begin{abstract} 

\end{abstract}
\maketitle

\section{Introduction}
We describe how to ``implement'' (really, prove) the inference phase only of several common layers with Plonkish arithmetization. The main tasks are implementing:
\begin{itemize}
\item fixed-point arithmetic using approximate quotient witnesses
\item quantization including handling rounding error
\item ReLu with lookup
\item Other nonlinearities (tanh, sigmoid) with lookup
\end{itemize}

Model parameters (weights and biases) are either fixed or treated as a private input to the circuit.  The general approach is to use PyTorch or similar for witness generation (recording each activation), producing an {\em activation trace}.  Quantization happens at inference time, and the activation trace should be i8s. The activation trace is then post-processed (adding supplementary witness such as approximate quotients) to produce a witness (sometimes we may need to do the quantization in the post-processing step).  This witness is then ready to be fed to the prover, which will show that given the weights and quantization scheme, each layer transition is correct. 

We avoid number-to-bits conversions entirely and use native field arithmetic to approximate floating-point arithmetic.  Here, care must be taken with overflow issues.  We can also use limb representations.


Basic idea: first, we apply a linear tensor transformation such as convolution or matrix multiply to transform the tensor (this uses a quadratic constraint for each output, and is usually a large sum), then apply a precomputed fused dropout-batchnorm-quantize-relu lookup table.  The linear transformation may require a larger accumulation number representation such as i16 or i32 (if necessary an i32 can be quantized in-proof to i16 as described in Section \ref{section_WQ} below). The lookup table is one-input one-output, i8 to i8 or i16 to i8 or possibly i32 to i8, and is computed by exhaustively passing all inputs into the PyTorch layer.  Typically only a few tables will be needed as the same transformation is repeated many times.


\section{Witness Quantization} \label{sectionWQ}
Suppose we have an i32 $x_{32}$, clip it to i16 range or scale it by dividing by $s_{32} = 2^{31}/2^{15} = 2^{16}$ or some other factor, and round to the nearest i16 to obtain $x_{16}$. If we perform that in witness generation, to avoid a $2^{32}$ lookup table, we would like to use constraints in $\FF_p[x_{32}, x_{16}, ...]$ to express that $x_{16} = {\rm round_{i16}}(x_{32}/s_{32})$.

Rationals of the form $x_{32} / 2^{16}$ have the same range $[-2^{15}, 2^{15}]$ as i16s but have 16 bits of precision. 
For example in the interval [-1,1] the i16s are -1,0,1 but there are $2^{16}$ rationals $x_{32} / 2^{16}$ from $-\frac{2^{16}}{2^{16}}$ to $\frac{2^{16}}{2^{16}}$ evenly spaced in the interval.
Anything in the numerator between 0 and $2^{15}$ inclusive rounds to zero, and between $2^{15}+1$ and $2^{16}$ inclusive rounds to 1.
Thus the numerator error is $s_{32} x_{16} - x_{32}$ and rounding is correct if the absolute value of this difference is less than $2^{15}$.

So in terms of constraints and lookups (with fixed scaling), we have three witnesses: the input $x_{32}$, the claimed output $x_{16}$, and the claimed error $e$. We require $e = s_{32} x_{16} - x_{32}$ and require $e,x_{16} \in [-2^{15},2^{15}]$ with lookups. 

We apply this only after a linear accumulation operation, so that we do not need to check that $x_{32} \in [-2^{31},2^{31}]$ because this can be proved at compile time from the fact that the input activations and weights are $i8$ and there are not too many of them (e.g. a dot product of two length-$2^8$ vectors of $i8$s can be at most $2^{22}$, and in most cases will likely be smaller than $2^{15}$).



\subsection{Witness Clip} \label{sectionWClip}
The clip(m) operation sets $x_{out}$ to $x_{in}$ if $x_{in} \in [-m,m]$ and to -m or m if smaller (larger).  The prover should provide, as well as $x_{in}$, the proposed $x_{out}$, and a value $c$ which is claimed to be  $c = 1$ if we clip to $m$, $0$ if no clipping occurs, or  $-1$ if we clip to $-m$.  So we have $c(c-1)(c+1) = 0$, $x_{out} = x_{in} - c*(x_{in} - m)$, $x_{out} \in [-m,m]$.


This could be done with a lookup table if we also have a larger max, i.e. numbers 0..m are left alone, m..M are set to m, and values above $M$ cause the proof to fail.
%% This requires a big table, but the second accumulation max can be fine (e.g. max matrix row size * max weight * max activation).  So e.g. if the max dot product size is 256, and we have i8 weights and activations, m=128, M=256*128*128=4mil.  quantizations use i32 for accumulators sometimes, so 65536/2 could work (i16).



\section{Clipped fixed-point arithmetic using prime fields}

Our goal is to implement in $\mathbb{F}_p$, in the sense of constraints and lookups, fixed-point arithmetic with a fixed denominator $d$, a fixed maximum weight and activation value $m$.  Thus $m/d$ is the largest number passed to the next layer and $-m/d$ the most negative, with $\pm 1/d$ the closest to zero.  There is also an accumulator max $M$ which is used for intermediate accumulators such as dot products (where using $m$ might cause too much error).  If an arithmetic operation would overflow before assignment (result in a numerator larger than $m$), then the resulting numerator should be $m$.  Note however that we might have an accumulator, followed by a rescaling, and the overflow check would only be post-rescaling.  A typical choice is i8 and i32, so $m=127$, $M=32,767$, and perhaps $d=32$, so activations are in the range $(-4,4)$.

A more serious kind of overflow occurs if an intermediate result is aliased due to the cyclic structure of the field, but with a 254 bit prime we have considerable room and checks may be useful to postpone.

For rational or floating point numbers in the valid range, the maximum rounding error of this scheme is $1/2d$, and we want to choose a closest representative.


Write $a_z$ for an integer value, $[a_p]$ for its image in $\mathbb{F}_p$.

Fix a denominator $d \in \ZZ_{>0}$ (typically 128 for an i8).  We wish to represent addition, subtraction, multiplication, and division of rational numbers, and specifically of fixed point rationals. A pair $(a_z,b_z)$ is a representative of the rational number $\frac{a_z}{b_z}$.


One strategy is simply to use a lookup table to implement all nonlinearities, including those that are consituents of other operations.  For example clip, rescaling in batchnorm layers, as well as relu.  E.g. just quantize the effect of the rescaling, then impl that lookup table.



\subsection{ReLu}

ReLu is five advice columns, r,k,z,a,b.  r is raw input (eg output of previous layer). Prover sets k to keep (1 if r>= 0, 0 o/w), z to zero (1 if r<0, 0 o/w). Constraints are a-rk, b+rz, k(1-k), z(1-z), k+z-1.  Range Lookups are a and  b in [0,65535].  Proof fails if overflows out of bounds. Does that sound roughly right?

i was thinking that the prover could directly witness (and look up) the result of ReLu:

lookup table:
| value | ReLu(value) |
-----------------------
|  -255 |      0      |
|  -244 |      0      |
|   ... |      0      |
|    0  |      0      |
|    1  |      1      |
|    2  |      2      |
|    3  |      3      |
|  ...  |     ...     |
|  255  |     255     |

witness:
|   r   | ReLu(r) |
[10:46 PM]
or is there some other use for the k, z parameters? (i think we could also just replace z with the expression 1 - k wherever we use it, instead of requiring the prover to witness it)


\subsection{Addition}
c = a + b
a and b have opposite signs and are both in the valid range, or have the same sign and are both in half the valid range (or just clip and use the fact that there are not enough addends to overflow p)


$(a_z,d) + (b_z,d) = (c_z, d)$ is faithful if $c_z = a_z + b_Z$.
$([a_z],[d]) + ([b_z],[d]) = ([c_z], [d])$ is faithful if $[c_z] = [a_z] + [b_Z]$ and $a_z + b_z$ doesn't overflow.  


\subsection{Additive inverse}


\subsection{Multiplication}


\subsection{Division}
We cannot use $[a_z]^{-1}$ for $a_z^{-1} \in \mathbb{Q}$. Instead we find a value $b_z$ in the allowed range so that $b_z/d$ approximates $1/a_z$, i.e. so that $\frac{b_z}{d} \cdot a_z \approx d$.

Rescaling error might be acceptable if the rescaling factor is small, e.g. we can only multiply by 1/128 up to 32/128,
[(round(128/i)*i)/128 - 1 for i in range(0,32) if i!=0]
[0.0, 0.0, 0.0078125, 0.0, 0.015625, -0.015625, -0.015625, 0.0, -0.015625, 0.015625, 0.03125, 0.03125, 0.015625, -0.015625, 0.0546875, 0.0, 0.0625, -0.015625, 0.0390625, -0.0625, -0.015625, 0.03125, 0.078125, -0.0625, -0.0234375, 0.015625, 0.0546875, 0.09375, -0.09375, -0.0625, -0.03125]
or use a number with a lot of factors like 360, round the rescaler to nearest perfectly invertible.


Suppose $a_z >0$, then we want $b_z$ such that $a_z b_z \approx d$ (since $(d,d)$ is one).  Since $d$ is a power of 2 and $a_z,b_z$ need not be, in general this will only be approximate.
For example if $d=8$, the number line is divided into eigths and the maximum possible error $|\frac{b_z}{8} - \frac{1}{a_Z}|$ is $\frac{1}{16}$.  Thus we want our circuit to require that $b_z/d$ is the closest such element to $\frac{1}{a_z}$.

So we want $b_z$ to be one of the at most two numbers minimizing the error, that is $d \in \{a_zb_z-1,a_zb_z,a_z b_z+1\}$; $a_z$ is an output so in VR, and we need $b_z$ in VR.
So the provided witness $b$ is the mulitplicative inverse of $a \in VR$ iff 
\[
(ab-1-d)(ab-d)(ab+1-d) \;\; \text{AND} b \in VR.
\]
d=8
m=16
a=13
1/13 rounds to 1/8
so b=1

\subsection{Dot product}


\section{Unsigned integer matrix-vector multiply}


\section{Range-checked integer matrix-vector multiply}

\section{ReLu}


\section{BatchNorm}


\section{Depthwise Convolution}


\section{Conv2d}


\section{MaxPool}


\section{Softmax}



\bibliographystyle{custom} 
\bibliography{bibfile}
\end{document}
